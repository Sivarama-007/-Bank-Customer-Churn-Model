# -*- coding: utf-8 -*-
"""BANKCUSTOMERCHURN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19i0sFPF6qgeLD2i_qc5mQ4YbEkvqzLjM

# **BANK CUSTOMER CHURN MODEL**

**LEARNING OBJECTIVES**

To predict whether a bank customer will churn (leave the bank) based on various demographic, financial, and behavioral data

**DATA SOUCE**

**The dataset can be found at the following link:**
[Bank Churn Modelling CSV](https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("https://raw.githubusercontent.com/YBI-Foundation/Dataset/main/Bank%20Churn%20Modelling.csv")
data.head()

data.info()

"""**DESCRIBE DATA**"""

data.describe()

data.info()

"""**DATA PREPROCESSING**"""

data.duplicated('CustomerId').sum()

data=data.set_index('CustomerId')

# Creating a new column for zero balance
data['Zero Balance'] = np.where(data['Balance'] > 0, 1, 0)

# Replace categorical variables with numerical values
data.replace({'Geography': {'France': 2, 'Germany': 1, 'Spain': 0}}, inplace=True)
data.replace({'Gender': {'Male': 0, 'Female': 1}}, inplace=True)
data.replace({'Num Of Products': {1: 0, 2: 1, 3: 1, 4: 1}}, inplace=True)

"""**Data Visualization**"""

data['Geography'].value_counts()

data['Gender'].value_counts()

data['Num Of Products'].value_counts()

data['Has Credit Card'].value_counts()

data['Is Active Member'].value_counts()

data.loc[(data['Balance']==0), 'Churn'].value_counts()

data['Zero Balance'] = np.where(data['Balance']>0,1,0)

data['Zero Balance'].hist()

data['Zero Balance'].hist()
plt.title('Distribution of Zero Balance')
plt.xlabel('Zero Balance (1 = Yes, 0 = No)')
plt.ylabel('Frequency')
plt.show()

data.groupby(['Churn','Geography']).count()

"""**DEFINE TARGET VARIABLE(y) and FEATURE VARIABLE (X)**"""

data.columns

x=data.drop(['Surname','Churn'],axis=1)
y=data['Churn']

x.shape,y.shape

"""**handling imabalance data**

Class Imbalance is a common problem in machine learning, espically in classification problems as machine learning algorithms are designed to maximise accuracy and reduce errors. if data set is imbalance then in such cases, just by predicting the majority class we get a pretty high accuracy, but fails to capture the minority class, which is most often the point of creating the model in the first place.like in

1.Fraud detection 2.Spam filtering 3.Disease screening 4.Online sales churn 5.Advertising click-throughs
"""

data['Churn'].value_counts()

sns.countplot(x='Churn',data=data)

x.shape,y.shape

"""**Random under sampling**"""

from imblearn.under_sampling import RandomUnderSampler

rus=RandomUnderSampler(random_state=2529)

x_rus,y_rus=rus.fit_resample(x,y)

x_rus.shape,y_rus.shape,x.shape,y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind='hist')

"""**Random over sampling**"""

from imblearn.over_sampling import RandomOverSampler

ros=RandomOverSampler(random_state=2529)

x_ros,y_ros=ros.fit_resample(x,y)

x_ros.shape,y_ros.shape,x.shape,y.shape

y.value_counts()

y_ros.value_counts()

y_ros.plot(kind='hist')

"""**TRAIN TEST SPLIT**"""

from sklearn.model_selection import train_test_split

#splitorigianldata
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=2529)

#splitrandomundersampledata
x_train_rus,x_test_rus,y_train_rus,y_test_rus=train_test_split(x_rus,y_rus,test_size=0.3,random_state=2529)

#splitrandomoversampledata
x_train_ros,x_test_ros,y_train_ros,y_test_ros=train_test_split(x_ros,y_ros,test_size=0.3,random_state=2529)

#standardizefeatures
from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

#STANDARDIZEoriginaldata
x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

#standardizerandomundersampledata
x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

#standardizerandomoversampledata
x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**MODELING**"""

#support vectortmachineclassifier
from sklearn.svm import SVC

svc=SVC()

svc.fit(x_train,y_train)

y_pred=svc.predict(x_test)

"""**MODEL EVALUATION**

model accuracy
"""

from sklearn.metrics import confusion_matrix,classification_report

confusion_matrix(y_test, y_pred)

print(classification_report(y_test,y_pred))

"""**Hyperparameter tunning**"""

from sklearn.model_selection import GridSearchCV

param_grid={'C':[0.1,1,10],'gamma':[1,0.1,0.01],'kernel':['rbf'],'class_weight':['balanced']}

grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)

grid.fit(x_train,y_train)

"""**Model with Random Under Sampling**"""

svc_rus=SVC()

svc_rus.fit(x_train_rus,y_train_rus)

y_pred_rus = svc_rus.predict(x_test_rus)

"""Model Accuracy"""

confusion_matrix(y_test_rus, y_pred_rus)

print(classification_report(y_test_rus, y_pred_rus))

"""**Hyperparameter Tunning**"""

param_grid = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf'], 'class_weight' : ['balanced']}

grid_rus = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2, cv = 2)

grid_rus.fit(x_train_rus, y_train_rus)

"""**Model with Random Over Sampling**"""

svc_ros = SVC()

svc_ros.fit(x_train_ros, y_train_ros)

y_pred_ros = svc_ros.predict(x_test_ros)

"""**Model Accuracy**"""

confusion_matrix(y_test_ros, y_pred_ros)

print(classification_report(y_test_ros, y_pred_ros))

"""**Hyperparameter Tunning**"""

param_grid = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf'], 'class_weight' : ['balanced']}

grid_ros = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2, cv = 2)

grid_ros.fit(x_train_ros, y_train_ros)

"""**Prediction**"""

print(grid.best_estimator_)

grid_predictions=grid.predict(x_test)

confusion_matrix(y_test,grid_predictions)

print(classification_report(y_test,grid_predictions))

"""Random under sampling"""

print(grid_rus.best_estimator_)

grid_pred_rus = grid_rus.predict(x_test_rus)

confusion_matrix(y_test_rus, grid_pred_rus)

print(classification_report(y_test_rus, grid_pred_rus))

"""Random over sampling"""

print(grid_ros.best_estimator_)

grid_pred_ros = grid_ros.predict(x_test_ros)

confusion_matrix(y_test_ros, grid_pred_ros)

print(classification_report(y_test_ros, grid_pred_ros))

"""**EXPLANATION**

The Bank Customer Churn Model aims to predict whether a customer will leave the bank based on several demographic, financial, and behavioral factors. The model was trained and evaluated using a combination of classification algorithms, random oversampling, undersampling, and hyperparameter tuning to address class imbalance.

Model Performance
Base Model:

The base model showed good accuracy (84%), with a high precision for non-churn customers (0.85). However, it struggled to predict churners (recall of 0.26), indicating the model's tendency to favor the majority class (non-churn).
Precision for churners was decent (0.82), but the low recall indicates that many actual churners were not identified, leading to an overall weighted F1-score of 0.81.
Random Undersampling:

After applying Random Undersampling, the model performed more balanced between the two classes, with a recall of 0.71 for churners and 0.75 for non-churners, resulting in a more balanced F1-score (0.72 for churners and 0.74 for non-churners). However, the overall accuracy dropped to 73%, as undersampling reduced the amount of available data for training.
Random Oversampling:

The model's performance improved significantly with Random Oversampling, achieving an accuracy of 92%. The recall for churners (0.97) was excellent, meaning that the model correctly identified most customers likely to churn. Similarly, non-churn recall remained high (0.86). This approach helped address the class imbalance, as reflected in the balanced F1-scores for both classes (0.92).
After Hyperparameter Tuning:

After tuning the hyperparameters, the model improved slightly with an accuracy of 80%, but the recall for churners remained low (0.41), indicating that while the model got better overall, it still struggled to correctly identify all churners. This suggests that further adjustments, such as different model architectures or additional features, might be necessary to improve recall.
Feature Importance and Insights
The features used in the model include:

Demographic Variables: 'Geography', 'Gender', 'Age'
Financial Variables: 'CreditScore', 'Balance', 'Estimated Salary'
Behavioral Variables: 'Tenure', 'Number of Products', 'Has Credit Card', 'Is Active Member', and 'Zero Balance'
From the analysis, certain features like Age, Balance, and Is Active Member had more predictive power in determining whether a customer would churn. Older customers and those with high balances but low engagement with the bank (inactive members) were more likely to churn.

Limitations and Considerations
Class Imbalance: The dataset exhibited significant class imbalance, with far more non-churners than churners. This was addressed using both random undersampling and oversampling techniques. Random oversampling yielded better performance in identifying churners, but it is essential to test the model on real-world data to ensure generalizability.

Feature Engineering: Further feature engineering, such as combining financial products or creating new variables (e.g., customer engagement scores), could improve the model's performance.

Model Selection: While the Support Vector Classifier (SVC) performed reasonably well, alternative algorithms such as XGBoost or Random Forest could be explored to potentially enhance predictive accuracy.

In summary, the Bank Customer Churn Model demonstrates that while predicting customer churn is feasible, identifying churners with high accuracy remains a challenge. Addressing class imbalance, refining features, and further optimizing the model could lead to more accurate and actionable predictions
"""